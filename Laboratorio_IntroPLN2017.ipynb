{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Introducción al Procesamiento de Lenguaje Natural 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Número de grupo: 13\n",
    "#### Integrantes:\n",
    "- Giovani Rondán, CI: 4.528.997-6\n",
    "- Santiago Behak, CI: 5.019.450-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Importación de los tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzaremos importando los tweets provenientes del archivo \"corpus_humor_training.csv\" usando la librería Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grondan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import re\n",
    "#import freeling\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import IPython.display as disp\n",
    "import os\n",
    "\n",
    "corpus = pandas.read_csv(\"corpus_humor_training.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los registros de este corpus están compuestos por varios datos además del propio texto del tweet. A continuación mostraremos la estructura del corpus y algunos de los tweets (que se encuentran en el atributo \"text\") a modo de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'text', 'account_id', 'n', '1', '2', '3', '4', '5'], dtype='object')\n",
      "\n",
      "\n",
      "-La semana pasada mi hijo hizo un triple salto mortal desde 20 metros de altura - ¿Es trapecista? -Era :(\n",
      "\n",
      "-Yo ya voy por mi segundo millón de dólares... -¿!Ah, si!? -Es que el primero nunca lo hice... #fb\n",
      "\n",
      "-Ayer fue mi cumpleaños y no me felicitaste - ¡FéÉLíCÍDáÁDÉéS! - ¿Qué haces? -Felicitarte con retraso.\n",
      "\n",
      "No es flojera, es un estado de ahorro de energía corporal :)\n",
      "\n",
      "- ¿Cómo te fue en matemática? -Vos sabes que soy muy pacífica - ¿Y eso qué tiene que ver? -No me gustan los problemas jajaja -Castigada - :(\n",
      "\n",
      "\"El pesimista se queja del viento; el optimista espera que cambie; el realista ajusta las velas\" Feliz miércoles.\n",
      "\n",
      "-¿Y tú desde cuando llevas pendiente? \r\n",
      "-Desde que mi mujer se lo encontró en el coche y le dije que era mío...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus.columns)\n",
    "print (\"\\n\")\n",
    "for text in corpus['text'][:7]:\n",
    "    print(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación porocedemos a analizar las características del corpus obtenido, para esto obtendremos algunos datos básicos tales como la cantidad total de tweets, la cantidad de atributos de los que disponemos, la cantidad de calificaciones de los 10 tweets más calificados y la cantidad total de calificaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de Tweets en el corpus: 12106\n",
      "Cantidad de atributos en el corpus: 9\n",
      "Lista de los diez tweets con más calificaciones:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>account_id</th>\n",
       "      <th>n</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>cantCalificaciones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>445969156437852161</td>\n",
       "      <td>—¿A dónde vas tan maquillada? —A una fiesta, m...</td>\n",
       "      <td>1518218509</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8158</th>\n",
       "      <td>446273545304162304</td>\n",
       "      <td>JAJAJAJAJAJA ¿TE ACUERDAS CUANDO... ah, no, tú...</td>\n",
       "      <td>229144847</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10238</th>\n",
       "      <td>446423336831053824</td>\n",
       "      <td>\"Tu invades mi cabeza\" —Juanita, 10 años, tien...</td>\n",
       "      <td>229144847</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11728</th>\n",
       "      <td>464188098780618752</td>\n",
       "      <td>#Chistetipico</td>\n",
       "      <td>124053720</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7849</th>\n",
       "      <td>447042317480763393</td>\n",
       "      <td>\"Es imposible\" dijo el orgullo; \"es arriesgado...</td>\n",
       "      <td>1518218509</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8966</th>\n",
       "      <td>448704341365366784</td>\n",
       "      <td>-Tenemos una relación seria. -¿Lleváis mucho t...</td>\n",
       "      <td>1518218509</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>446934186889211905</td>\n",
       "      <td>Hablo 3 idiomas: español, sarcasmos e indirectas.</td>\n",
       "      <td>1518218509</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7037</th>\n",
       "      <td>445481775133777921</td>\n",
       "      <td>Molestar a alguien solo porque te gusta ver co...</td>\n",
       "      <td>1518218509</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>445669834211069952</td>\n",
       "      <td>A mi también me castigaron por reírme mientras...</td>\n",
       "      <td>1518218509</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4306</th>\n",
       "      <td>447613590992732160</td>\n",
       "      <td>—En mis tiempos... —Sí, sí abuela, como digas,...</td>\n",
       "      <td>1518218509</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "5879   445969156437852161  —¿A dónde vas tan maquillada? —A una fiesta, m...   \n",
       "8158   446273545304162304  JAJAJAJAJAJA ¿TE ACUERDAS CUANDO... ah, no, tú...   \n",
       "10238  446423336831053824  \"Tu invades mi cabeza\" —Juanita, 10 años, tien...   \n",
       "11728  464188098780618752                                     #Chistetipico    \n",
       "7849   447042317480763393  \"Es imposible\" dijo el orgullo; \"es arriesgado...   \n",
       "8966   448704341365366784  -Tenemos una relación seria. -¿Lleváis mucho t...   \n",
       "1117   446934186889211905  Hablo 3 idiomas: español, sarcasmos e indirectas.   \n",
       "7037   445481775133777921  Molestar a alguien solo porque te gusta ver co...   \n",
       "9273   445669834211069952  A mi también me castigaron por reírme mientras...   \n",
       "4306   447613590992732160  —En mis tiempos... —Sí, sí abuela, como digas,...   \n",
       "\n",
       "       account_id   n  1  2  3  4  5  cantCalificaciones  \n",
       "5879   1518218509   2  6  3  2  6  2                  21  \n",
       "8158    229144847  13  2  1  4  0  0                  20  \n",
       "10238   229144847   4  6  4  3  2  0                  19  \n",
       "11728   124053720  17  0  0  0  1  0                  18  \n",
       "7849   1518218509   6  6  1  1  3  0                  17  \n",
       "8966   1518218509   1  2  4  3  3  4                  17  \n",
       "1117   1518218509   6  4  1  4  0  1                  16  \n",
       "7037   1518218509  14  1  0  1  0  0                  16  \n",
       "9273   1518218509  11  3  0  0  2  0                  16  \n",
       "4306   1518218509   4  1  3  4  2  1                  15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cantTweets = len(corpus)\n",
    "cantAtributos = len(corpus.columns)\n",
    "\n",
    "# Imprimimos la cantidad total de tweets y la cantidad de atributos\n",
    "print (\"Cantidad de Tweets en el corpus: \" + str(cantTweets))\n",
    "print (\"Cantidad de atributos en el corpus: \" + str(cantAtributos))\n",
    "\n",
    "# Imprimimos la cantidad de calificaciones de los 10 tweets mas calificados y la cantidad total de calificaciones\n",
    "totalCalificaciones = 0\n",
    "contador = 0\n",
    "corpus[\"cantCalificaciones\"] = [0]*len(corpus)\n",
    "for i in range(0, 12106):\n",
    "    calificacionesTweet =corpus.loc[i, \"n\"] + corpus.loc[i, \"1\"]  + corpus.loc[i, \"2\"] + corpus.loc[i, \"3\"] + corpus.loc[i, \"4\"] + corpus.loc[i, \"5\"]\n",
    "    totalCalificaciones += calificacionesTweet\n",
    "    corpus.loc[i, \"cantCalificaciones\"] = calificacionesTweet\n",
    "\n",
    "print (\"Lista de los diez tweets con más calificaciones:\\n\")\n",
    "disp.display(corpus.sort_values(by = ['cantCalificaciones'], ascending = False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro dato que puede resultar interesante es la cantidad de calificaciones por valor (las calificaciones no humorísiticas serán contadas con el 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzErifwGLihAYRirG3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x287f2bf8208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Se realiza una gráfica de cantidad de comentarios en función de su clasificación\n",
    "calificacionesPorValor = [corpus[\"n\"].sum(), corpus[\"1\"].sum(), corpus[\"2\"].sum(), corpus[\"3\"].sum(), corpus[\"4\"].sum(), corpus[\"5\"].sum()]\n",
    "valoresCalificaciones = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "pos = arange(len(valoresCalificaciones)) + 0.5 \n",
    "\n",
    "figure(1)\n",
    "barh(pos,calificacionesPorValor, align='center')\n",
    "yticks(pos, valoresCalificaciones)\n",
    "xlabel('Cantidad de calificaciones')\n",
    "ylabel(u'Calificación')\n",
    "title(u'Cantidad de calificaciones por valor')\n",
    "grid(True)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en la gráfica la mayoría de las calificaciones corresponden al valor 0, o sea, como calificaciones no humorísticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Preprocesmiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En primer lugar procederemos a eleminar las columnas del corpus que consideramos innecesarias. Eliminaremos la columna de la id del tweet ya que este es un número autogenerado aleatorio que no debería aportar información relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'account_id', 'n', '1', '2', '3', '4', '5',\n",
      "       'cantCalificaciones'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "if \"id\" in corpus.columns:\n",
    "    del corpus[\"id\"]\n",
    "print (corpus.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez filtradas las columnas del corpus nos disponemos a filtrar los tweets que tienen menos de tres calificaciones dado que los mismos no cuentan con una cantidad significativa de calificaciones como para ser evaluados. Además se eliminarán los hashtags de los textos de los tweets como se pide en la letra y agregaremos una nueva columna que determina si un tweet es humorístico en función del número de calificaciones no humorísticas en relación al total de calificaciones del tweet. Si la cantidad de calificaciones humorísticas es mayor o igual a la suma del resto de calificaciones el tweet se considerará no humorístico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>account_id</th>\n",
       "      <th>n</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>humoristico</th>\n",
       "      <th>cantCalificaciones</th>\n",
       "      <th>mediana</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-La semana pasada mi hijo hizo un triple salto...</td>\n",
       "      <td>118161896</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-Yo ya voy por mi segundo millón de dólares......</td>\n",
       "      <td>132679073</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-Ayer fue mi cumpleaños y no me felicitaste - ...</td>\n",
       "      <td>118161896</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No es flojera, es un estado de ahorro de energ...</td>\n",
       "      <td>1518218509</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- ¿Cómo te fue en matemática? -Vos sabes que s...</td>\n",
       "      <td>118161896</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-Compadre, su hija antes me daba como por las ...</td>\n",
       "      <td>132679073</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Había una vez una tortuguita que fue a su prim...</td>\n",
       "      <td>142482558</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Una novia que sea tan delicada como Neymar, es...</td>\n",
       "      <td>142482558</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>¿Qué le dice el Nesquik a la leche? ¡Te voy a ...</td>\n",
       "      <td>132679073</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Si oscar se queda me pego un tiro con un banan...</td>\n",
       "      <td>574848706</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>- ¿Cual es tu nombre? -Pikachu Rodríguez, ¿y e...</td>\n",
       "      <td>118161896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  account_id  n  1  2  3  \\\n",
       "0   -La semana pasada mi hijo hizo un triple salto...   118161896  0  1  1  0   \n",
       "1   -Yo ya voy por mi segundo millón de dólares......   132679073  2  1  0  0   \n",
       "2   -Ayer fue mi cumpleaños y no me felicitaste - ...   118161896  0  1  1  1   \n",
       "3   No es flojera, es un estado de ahorro de energ...  1518218509  1  1  1  0   \n",
       "4   - ¿Cómo te fue en matemática? -Vos sabes que s...   118161896  2  0  0  1   \n",
       "5   -Compadre, su hija antes me daba como por las ...   132679073  1  1  0  0   \n",
       "6   Había una vez una tortuguita que fue a su prim...   142482558  1  2  1  0   \n",
       "7   Una novia que sea tan delicada como Neymar, es...   142482558  2  0  0  0   \n",
       "8   ¿Qué le dice el Nesquik a la leche? ¡Te voy a ...   132679073  2  1  0  0   \n",
       "9   Si oscar se queda me pego un tiro con un banan...   574848706  6  1  1  0   \n",
       "10  - ¿Cual es tu nombre? -Pikachu Rodríguez, ¿y e...   118161896  0  0  1  0   \n",
       "\n",
       "    4  5 humoristico cantCalificaciones  mediana  \n",
       "0   0  1        True                  3      2.0  \n",
       "1   0  0       False                  3      0.0  \n",
       "2   0  1        True                  4      3.0  \n",
       "3   0  0        True                  3      1.0  \n",
       "4   0  0       False                  3      0.0  \n",
       "5   1  0        True                  3      1.0  \n",
       "6   0  0        True                  4      1.0  \n",
       "7   1  0       False                  3      0.0  \n",
       "8   1  0        True                  4      1.0  \n",
       "9   1  0       False                  9      0.0  \n",
       "10  2  0        True                  3      4.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tweets que quedan en el corpus luego del filtrado: 3438\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import math\n",
    "\n",
    "corpus[\"humoristico\"] = [False]*len(corpus)\n",
    "corpus_filtrado = pandas.DataFrame(columns = ['text', 'account_id', 'n', '1', '2', '3', '4', '5', 'humoristico', 'cantCalificaciones', 'mediana'])\n",
    "#corpus_filtrado = corpus[corpus.n + corpus.columns[4] + corpus.columns[5] + corpus.columns[6] + corpus.columns[7] + corpus.columns[8] >= 3]\n",
    "total = 0\n",
    "for i in range(0, 12106):\n",
    "    contador = corpus.loc[i, \"cantCalificaciones\"]\n",
    "    #eliminamos los hashtags\n",
    "    corpus.loc[i, \"text\"] = re.sub(r\"#\\S+\\s*\", \"\", corpus.loc[i, \"text\"])\n",
    "    #definimos si un tweet es humoristico o no segun los votos\n",
    "    if(contador/2 >= corpus.loc[i, \"n\"]):\n",
    "        corpus.loc[i, \"humoristico\"] = True\n",
    "    #calculo mediana\n",
    "    califications= []\n",
    "    califications += [0]* corpus.loc[i, \"n\"]\n",
    "    califications += [1]* corpus.loc[i, \"1\"]\n",
    "    califications += [2]* corpus.loc[i, \"2\"]\n",
    "    califications += [3]* corpus.loc[i, \"3\"]\n",
    "    califications += [4]* corpus.loc[i, \"4\"]\n",
    "    califications += [5]* corpus.loc[i, \"5\"]\n",
    "    mediana = statistics.median(califications)\n",
    "    if (trunc(mediana) < mediana):\n",
    "        mediana = trunc(mediana) +1\n",
    "    corpus.loc[i,\"mediana\"] = mediana\n",
    "    #filtramos los tweets que tienen menos de 3 votos\n",
    "    if contador >= 3:\n",
    "        corpus_filtrado.loc[total] = [corpus.loc[i, \"text\"], corpus.loc[i, \"account_id\"], corpus.loc[i, \"n\"], corpus.loc[i, \"1\"], corpus.loc[i, \"2\"], corpus.loc[i, \"3\"], corpus.loc[i, \"4\"], corpus.loc[i, \"5\"], corpus.loc[i, \"humoristico\"], corpus.loc[i, \"cantCalificaciones\"],corpus.loc[i,\"mediana\"]]\n",
    "        total += 1\n",
    "        \n",
    "#columna 3 -> n, 4 -> 1, 5 -> 2, 6 -> 3, 7 -> 4, 8 -> 5\n",
    "\n",
    "disp.display(corpus_filtrado.loc[0:10, :])\n",
    "print (\"Cantidad de tweets que quedan en el corpus luego del filtrado: \" + str(len(corpus_filtrado)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Separación de los datos en conjunto de train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación dividiremos el corpus restante en un conjunto de train y en otro de test. En principio usaremos un 80% de los datos para el entrenamiento y un 20% para el testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tweets en el conjunto de entrenamiento: 2750\n",
      "Cantidad de tweets en el conjunto de testeo: 688\n"
     ]
    }
   ],
   "source": [
    "corpus_train, corpus_test = train_test_split(corpus_filtrado, test_size=0.2)\n",
    "\n",
    "print (\"Cantidad de tweets en el conjunto de entrenamiento: \" + str(len(corpus_train)))\n",
    "print (\"Cantidad de tweets en el conjunto de testeo: \" + str(len(corpus_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Clasificador binario con tokenizador y POS tag de Freeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1 Tokenización del corpus usando Freeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez separados el conjunto de train y test procedemos a tokenizar los tweets provenientes del conjunto train. Para esto usaremos la librería Freeling y NLTK. Primero definiremos una función que tokeniza un corpus usando freeling y a la que se le pueden pasar filtros para eliminar palabras basados en el postag de Freeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import datetime\n",
    "import traceback as tb\n",
    "\n",
    "def filtrar_corpus(corpus, filtros, train, lemma = False, usuario = False, with_tag = False, mediana = False):\n",
    "    result = []\n",
    "    palabras = \"\"\n",
    "    finTweetH = \"13grupoPLN\"\n",
    "    finTweetNH = \"13grupoIPLN\"\n",
    "    \n",
    "    for index, tweet in corpus.iterrows():\n",
    "        palabras+= tweet[\"text\"]\n",
    "        palabras+= \". \"\n",
    "        if(tweet[\"humoristico\"]):\n",
    "            palabras+= finTweetH\n",
    "        else:\n",
    "            palabras+= finTweetNH\n",
    "        palabras+= \". \"\n",
    "\n",
    "    p = Popen(\"C:\\grondan\\PLN2/FreelingWindows/bin/analyzer.bat -f C:\\grondan\\PLN2/FreelingWindows/data/config/es.cfg\", shell = True, stdout=PIPE, stdin=PIPE, stderr=STDOUT)\n",
    "    stdout = p.communicate(input=palabras.encode())[0]\n",
    "    iterator = -1\n",
    "    tweets = stdout.decode().split('\\r\\n')\n",
    "    for index, row in corpus.iterrows():\n",
    "        iterator += 1\n",
    "        tokens = tweets[iterator].split(' ')\n",
    "        diccionario = {}\n",
    "        while (tokens[0] != finTweetH and tokens[0] != finTweetNH):\n",
    "            if(tokens[0] != ''):\n",
    "                if(lemma):\n",
    "                    token = tokens[1]\n",
    "                else:\n",
    "                    token = tokens[0]\n",
    "                tag = tokens[2]\n",
    "                flag = True\n",
    "                for filtro in filtros:\n",
    "                    tag_aux = tag[0:len(filtro)]\n",
    "                    if (tag_aux == filtro):\n",
    "                        flag = False\n",
    "                        break\n",
    "                if flag:\n",
    "                    if(token in diccionario):\n",
    "                        diccionario[token] += 1\n",
    "                    else:\n",
    "                        diccionario[token] = 1\n",
    "                    if(with_tag):\n",
    "                        if(tag in diccionario):\n",
    "                            diccionario[tag] += 1\n",
    "                        else:\n",
    "                            diccionario[tag] = 1\n",
    "            tokens = tweets[iterator].split(' ')\n",
    "            iterator += 1\n",
    "        if(usuario):\n",
    "            hola = row[\"account_id\"]\n",
    "            diccionario[hola] = 1\n",
    "        if(train):\n",
    "            if(mediana):\n",
    "                result.append((diccionario, row[\"mediana\"]))\n",
    "            else:\n",
    "                result.append((diccionario, tokens[0] == finTweetH))\n",
    "        else:\n",
    "            result.append(diccionario)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2 Pruebas de clasificadores y filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta función probaremos dos clasificadores (Naive Bayes y Máxima Entropía) y un conjunto de filtros para eliminar tokens que en principio no parecen tan interesantes. Los filtros que consideramos probar son: \"F\" (signos de puntuacion), \"D\" (determinantes), \"P\" (pronombres), \"S\" (aposiciones) y \"Z\" (números). Probaremos usar todas las combinaciones de clasificadores y filtros y veremos cuales dan mejores resultados. Por ahora los únicos features que usaremos para la clasificación serán la forma de las palabras, luego agregaremos más features una vez que obtengamos la configuración ideal. (Atención, la siguiente cell demora mucho en ejecutarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entrene\n",
      "Matriz de confusión de Naive Bayes con filtros: F\n",
      "  |   0   1   2 |\n",
      "--+-------------+\n",
      "0 |<203> 55   . |\n",
      "1 | 128<302>  . |\n",
      "2 |   .   .  <.>|\n",
      "--+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Accuracy: 0.7340116279069767\n",
      "Matriz de confusión de Maxima Entropia con filtros: F\n",
      "  |   0   1   2 |\n",
      "--+-------------+\n",
      "0 |<230> 69   . |\n",
      "1 | 101<288>  . |\n",
      "2 |   .   .  <.>|\n",
      "--+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Accuracy: 0.752906976744186\n",
      "entrene\n",
      "Matriz de confusión de Naive Bayes con filtros: D\n",
      "  |   0   1   2 |\n",
      "--+-------------+\n",
      "0 |<209> 49   . |\n",
      "1 | 122<308>  . |\n",
      "2 |   .   .  <.>|\n",
      "--+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Accuracy: 0.751453488372093\n",
      "Matriz de confusión de Maxima Entropia con filtros: D\n",
      "  |   0   1   2 |\n",
      "--+-------------+\n",
      "0 |<249> 78   . |\n",
      "1 |  82<279>  . |\n",
      "2 |   .   .  <.>|\n",
      "--+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Accuracy: 0.7674418604651163\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-67030e1f0048>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mfilter_combinations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfilter_combination\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilter_combinations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mcorpus_pos_tagging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltrar_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_combination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mclassifiers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mclassifiers\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_pos_tagging\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Naive Bayes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-ba14ae7b8e29>\u001b[0m in \u001b[0;36mfiltrar_corpus\u001b[1;34m(corpus, filtros, train, lemma, usuario, with_tag, mediana)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\grondan\\PLN2/FreelingWindows/bin/analyzer.bat -f C:\\grondan\\PLN2/FreelingWindows/data/config/es.cfg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mstdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpalabras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m                 \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1086\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m             \u001b[1;31m# Wait for the reader threads, or time out.  If we time out, the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_stdin_write\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mBrokenPipeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m                 \u001b[1;32mpass\u001b[0m  \u001b[1;31m# communicate() must ignore broken pipe errors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics.scores import *\n",
    "import itertools\n",
    "\n",
    "max_accuracy = 0\n",
    "filtros = [\"F\", \"D\", \"P\", \"S\", \"Z\"]\n",
    "for i in range(1, len(filtros) + 1):\n",
    "    filter_combinations = itertools.combinations(filtros, i)\n",
    "    for filter_combination in filter_combinations:\n",
    "        corpus_pos_tagging = filtrar_corpus(corpus_train, filter_combination, True)\n",
    "        classifiers = []\n",
    "        classifiers += [[nltk.classify.NaiveBayesClassifier.train(corpus_pos_tagging), \"Naive Bayes\"]]\n",
    "        classifiers += [[nltk.classify.MaxentClassifier.train(corpus_pos_tagging, max_iter=8,trace=0), \"Maxima Entropia\"]]\n",
    "\n",
    "        print(\"entrene\")\n",
    "        #for classifier in classifiers:\n",
    "        #    classifier[0].show_most_informative_features()\n",
    "        salida = []\n",
    "        corpus_test_tokenizado = filtrar_corpus(corpus_test, filter_combination, False)\n",
    "        for classifier in classifiers:\n",
    "            salidaClasificador = []\n",
    "            for tweet in corpus_test_tokenizado:\n",
    "                # Se obtiene la clasificacion del algoritmo para el comentario\n",
    "                try:\n",
    "                    clasificacion = classifier[0].classify(tweet)\n",
    "                    salidaClasificador.append(clasificacion)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            salida = corpus_test.loc[:, \"humoristico\"]\n",
    "            cm = nltk.ConfusionMatrix(salidaClasificador, salida)\n",
    "            print(\"Matriz de confusión de \" + classifier[1] + \" con filtros: \" + '-'.join(filter_combination))\n",
    "            print(cm)\n",
    "            acc = accuracy(salidaClasificador, salida)\n",
    "            print(\"Accuracy: \" + str(acc))\n",
    "            if acc > max_accuracy:\n",
    "                max_accuracy = acc\n",
    "                best_combination = filter_combination\n",
    "                \n",
    "print(\"\\n Mejor combinacion: \" + '-'.join(best_combination) + \"\\n Mejor accuracy: \" + str(max_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de varias ejecuciones con diferentes conjuntos de test comprobamos que Máxima Entropía es claramente mejor que Naive Bayes para este clasificador: en todos los casos probados Máxima Entropía obtiene entre 2 y 5 puntos más de precisión que Naive Bayes. Además se puede ver que existen variaciones significativas de precisión para los diferentes filtros (en general existe una diferencia de 8 puntos entre la mejor y la peor combinación), en particular la precisión baja de forma apreciable en las combinaciones que poseen el filtro \"F\" (correspondiente a los signos de puntuación). De hecho si se listan los features más significativos de Naive Bayes sin aplicar ningún filtro se puede ver que de hecho el guión \"-\" es el token que aporta más información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_pos_tagging = filtrar_corpus(corpus_train, [], True)\n",
    "classifier = nltk.classify.NaiveBayesClassifier.train(corpus_pos_tagging)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probablemente esto se debe a que el guión se usa para simular diálogos, los cuáles son muy frecuentes en los chistes y los relatos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3 Pruebas con nuevos features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prueba anterior nos permite obtener entre un 70% y un 75% de precisión, así que intentaremos mejorar estos resultados agregando nuevos features como pueden ser la id del usuario, usar el lema de las palabras en lugar de su forma y el pos-tag devuelto por Freeling para los tokens. Para estas pruebas mantendremos el clasificador de Máxima Entropía además de todos los filtros menos el de los signos de puntuación porque, como ya discutimos, estos fueron los que dieron mejores resultados en las pruebas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics.scores import *\n",
    "\n",
    "best_accuracy = 0\n",
    "\n",
    "for i in range(0, 8):\n",
    "    aux_i = i\n",
    "    bool1 = (1 == (aux_i % 2))\n",
    "    aux_i = aux_i // 2\n",
    "    bool2 = (1 == (aux_i % 2))\n",
    "    aux_i = aux_i // 2\n",
    "    bool3 = (1 == (aux_i % 2))\n",
    "    corpus_pos_tagging = filtrar_corpus(corpus_train, [\"P\", \"S\", \"Z\", \"D\"], True, bool1, bool2, bool3)\n",
    "    clf = nltk.classify.MaxentClassifier.train(corpus_pos_tagging, max_iter=8,trace=0)\n",
    "    corpus_test_tokenizado = filtrar_corpus(corpus_test, [\"P\", \"S\", \"Z\", \"D\"], False, bool1, bool2, bool3)\n",
    "    clasificacionCLF = []\n",
    "    clasificacionOficial = []\n",
    "    for tweet in corpus_test_tokenizado:\n",
    "        #Clasificamos los tweets del corpus de test\n",
    "        clasificacion = clf.classify(tweet)\n",
    "        clasificacionCLF.append(clasificacion)\n",
    "\n",
    "    #Obtenemos las clasificaciones oficiales de los tweets del conjunto de test para compararlos con los del clasificador\n",
    "    clasificacionOficial = corpus_test.loc[:, \"humoristico\"]\n",
    "    cm = nltk.ConfusionMatrix(clasificacionCLF, clasificacionOficial)\n",
    "    print(\"Matriz de confusión con features: \" + str(bool1) + \"-\" + str(bool2) + \"-\" + str(bool3))\n",
    "    print(cm)\n",
    "    acc = accuracy(clasificacionCLF, clasificacionOficial)\n",
    "    print(\"Accuracy: \" + str(acc))\n",
    "    \n",
    "    if(acc > best_accuracy):\n",
    "        best_accuracy = acc\n",
    "        best_features = [bool1, bool2, bool3]\n",
    "\n",
    "print(\"Mejor combinacion de features: \" + str(best_features[0]) + \"-\" + str(best_features[1]) + \"-\" + str(best_features[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados indican que los mejores resultados se obtienen cuando todas las features mencionadas (usuario, lema de la palabra, y pos-tags de Freeling) están presentes. En general se obtiene una mejora de 4 o 5 puntos con respecto a la prueba anterior y de esta forma alcanzamos entre 74% y 79% de precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Clasificador por mediana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continiación construiremos un nuevo clasificador sobre si los tweets que forman parte del corpus son humorísticos. Para esto procederemos evaluando la mediana de las clasificaciones obtenidas, y consideraremos como humorísticos a los tweets que tengan una mediana mayor o igual a 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1- Cálculo de mediana\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construiremos un clasificador que pronostique la mediana de las clasificaciones sobre el grado de humor del tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión de Maxima Entropia con filtros: Z\n",
      "    |   0   1   2   3   4   5   6   7   8 |\n",
      "    |   .   .   .   .   .   .   .   .   . |\n",
      "    |   0   0   0   0   0   0   0   0   0 |\n",
      "----+-------------------------------------+\n",
      "0.0 |<311> 61  56  42  24   5   .   .   . |\n",
      "1.0 |   5  <9>  8  10   6   .   .   .   . |\n",
      "2.0 |  10  11 <13> 16   5   3   .   .   . |\n",
      "3.0 |   4  12  17 <21> 24   1   .   .   . |\n",
      "4.0 |   1   4   4   2  <2>  .   .   .   . |\n",
      "5.0 |   .   .   .   1   .  <.>  .   .   . |\n",
      "6.0 |   .   .   .   .   .   .  <.>  .   . |\n",
      "7.0 |   .   .   .   .   .   .   .  <.>  . |\n",
      "8.0 |   .   .   .   .   .   .   .   .  <.>|\n",
      "----+-------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Accuracy: 0.517441860465\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics.scores import *\n",
    "\n",
    "corpus_pos_tagging = filtrar_corpus(corpus_train, [\"P\", \"S\", \"Z\",\"D\"], True, True, True, True,True)\n",
    "clf = nltk.classify.MaxentClassifier.train(corpus_pos_tagging, max_iter=8,trace=0)\n",
    "corpus_test_tokenizado = filtrar_corpus(corpus_test, [\"P\", \"S\", \"Z\",\"D\"], False, True, True, True)\n",
    "salidaClasificador = []\n",
    "salida = []\n",
    "cont = 0\n",
    "for tweet in corpus_test_tokenizado:\n",
    "    # Se obtiene la clasificacion del algoritmo para el comentario\n",
    "    clasificacion = clf.classify(tweet)\n",
    "    salidaClasificador.append(clasificacion)\n",
    "salida = corpus_test.loc[:, \"mediana\"]\n",
    "cm = nltk.ConfusionMatrix(salidaClasificador, salida)\n",
    "print(\"Matriz de confusión de Maxima Entropia con filtros: Z\")\n",
    "print(cm)\n",
    "acc = accuracy(salidaClasificador, salida)\n",
    "print(\"Accuracy: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
