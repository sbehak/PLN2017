{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Introducción al Procesamiento de Lenguaje Natural 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Número de grupo: 13\n",
    "#### Integrantes:\n",
    "- Giovani Rondán, CI: 4.528.997-6\n",
    "- Santiago Behak, CI: 5.019.450-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Importación de los tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzaremos importando los tweets provenientes del archivo \"corpus_humor_training.csv\" usando la librería Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grondan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_freeling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\grondan\\PLN2\\2017\\PLN2017\\freeling.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_freeling'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named '_freeling'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aaa55debff16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfreeling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpylab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\grondan\\PLN2\\2017\\PLN2017\\freeling.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_freeling\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\grondan\\PLN2\\2017\\PLN2017\\freeling.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_freeling'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mimport\u001b[0m \u001b[0m_freeling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_freeling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfp\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_freeling'"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import re\n",
    "#import freeling\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import IPython.display as disp\n",
    "import os\n",
    "\n",
    "corpus = pandas.read_csv(\"corpus_humor_training.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los registros de este corpus están compuestos por varios datos además del propio texto del tweet. A continuación mostraremos la estructura del corpus y algunos de los tweets (que se encuentran en el atributo \"text\") a modo de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.columns)\n",
    "print \"\\n\"\n",
    "for text in corpus['text'][:7]:\n",
    "    print(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación porocedemos a analizar las características del corpus obtenido, para esto obtendremos algunos datos básicos tales como la cantidad total de tweets, la cantidad de atributos de los que disponemos, la cantidad de calificaciones de los 10 tweets más calificados y la cantidad total de calificaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cantTweets = len(corpus)\n",
    "cantAtributos = len(corpus.columns)\n",
    "\n",
    "# Imprimimos la cantidad total de tweets y la cantidad de atributos\n",
    "print (\"Cantidad de Tweets en el corpus: \" + str(cantTweets))\n",
    "print (\"Cantidad de atributos en el corpus: \" + str(cantAtributos))\n",
    "\n",
    "# Imprimimos la cantidad de calificaciones de los 10 tweets mas calificados y la cantidad total de calificaciones\n",
    "totalCalificaciones = 0\n",
    "contador = 0\n",
    "corpus[\"cantCalificaciones\"] = [0]*len(corpus)\n",
    "for i in range(0, 12106):\n",
    "    calificacionesTweet =corpus.loc[i, \"n\"] + corpus.loc[i, \"1\"]  + corpus.loc[i, \"2\"] + corpus.loc[i, \"3\"] + corpus.loc[i, \"4\"] + corpus.loc[i, \"5\"]\n",
    "    totalCalificaciones += calificacionesTweet\n",
    "    corpus.loc[i, \"cantCalificaciones\"] = calificacionesTweet\n",
    "\n",
    "print (\"Lista de los diez tweets con más calificaciones:\\n\")\n",
    "disp.display(corpus.sort_values(by = ['cantCalificaciones'], ascending = False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro dato que puede resultar interesante es la cantidad de calificaciones por valor (las calificaciones no humorísiticas serán contadas con el 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Se realiza una gráfica de cantidad de comentarios en función de su clasificación\n",
    "calificacionesPorValor = [corpus[\"n\"].sum(), corpus[\"1\"].sum(), corpus[\"2\"].sum(), corpus[\"3\"].sum(), corpus[\"4\"].sum(), corpus[\"5\"].sum()]\n",
    "valoresCalificaciones = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "pos = arange(len(valoresCalificaciones)) + 0.5 \n",
    "\n",
    "figure(1)\n",
    "barh(pos,calificacionesPorValor, align='center')\n",
    "yticks(pos, valoresCalificaciones)\n",
    "xlabel('Cantidad de calificaciones')\n",
    "ylabel(u'Calificación')\n",
    "title(u'Cantidad de calificaciones por valor')\n",
    "grid(True)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en la gráfica la mayoría de las calificaciones corresponden al valor 0, o sea, como calificaciones no humorísticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Preprocesmiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En primer lugar procederemos a eleminar las columnas del corpus que consideramos innecesarias. Eliminaremos las columnas tanto de la id del tweet como la de la id del usuario. La id del tweet es un número autogenerado que no aporta información relevante ya que es completamente aleatoria y la id del usuario, si bien puede llegar a relacionarse con usuarios que tienden a emitir tweets de carácter más o menos humorístico, no es un factor concluyente para determinar el nivel de humor de un tweet particular (un mismo usuario puede hacer tweets humorísticos y no humorísticos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"id\" in corpus.columns:\n",
    "    del corpus[\"id\"]\n",
    "if \"account_id\" in corpus.columns:\n",
    "    del corpus[\"account_id\"]\n",
    "print corpus.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez filtradas las columnas del corpus nos disponemos a filtrar los tweets que tienen menos de tres calificaciones dado que los mismos no cuentan con una cantidad significativa de calificaciones como para ser evaluados. Además se eliminarán los hashtags de los textos de los tweets como se pide en la letra y agregaremos una nueva columna que determina si un tweet es humorístico en función del número de calificaciones no humorísticas en relación al total de calificaciones del tweet. Si la cantidad de calificaciones humorísticas es mayor o igual a la suma del resto de calificaciones el tweet se considerará no humorístico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus[\"humoristico\"] = [False]*len(corpus)\n",
    "corpus_filtrado = pandas.DataFrame(columns = ['text', 'n', '1', '2', '3', '4', '5', 'humoristico', 'cantCalificaciones'])\n",
    "#corpus_filtrado = corpus[corpus.n + corpus.columns[4] + corpus.columns[5] + corpus.columns[6] + corpus.columns[7] + corpus.columns[8] >= 3]\n",
    "total = 0\n",
    "for i in range(0, 12106):\n",
    "    contador = corpus.loc[i, \"cantCalificaciones\"]\n",
    "    #eliminamos los hashtags\n",
    "    corpus.loc[i, \"text\"] = re.sub(r\"#\\S+\\s*\", \"\", corpus.loc[i, \"text\"])\n",
    "    #definimos si un tweet es humoristico o no segun los votos\n",
    "    if(contador/2 >= corpus.loc[i, \"n\"]):\n",
    "        corpus.loc[i, \"humoristico\"] = True\n",
    "    #filtramos los tweets que tienen menos de 3 votos\n",
    "    if contador >= 3:\n",
    "        corpus_filtrado.loc[total] = [corpus.loc[i, \"text\"], corpus.loc[i, \"n\"], corpus.loc[i, \"1\"], corpus.loc[i, \"2\"], corpus.loc[i, \"3\"], corpus.loc[i, \"4\"], corpus.loc[i, \"5\"], corpus.loc[i, \"humoristico\"], corpus.loc[i, \"cantCalificaciones\"]]\n",
    "        total += 1\n",
    "        \n",
    "#columna 3 -> n, 4 -> 1, 5 -> 2, 6 -> 3, 7 -> 4, 8 -> 5\n",
    "\n",
    "disp.display(corpus_filtrado.loc[0:10, :])\n",
    "print \"Cantidad de tweets que quedan en el corpus luego del filtrado: \" + str(len(corpus_filtrado))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Separación de los datos en conjunto de train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación dividiremos el corpus restante en un conjunto de train y en otro de test. En principio usaremos un 80% de los datos para el entrenamiento y un 20% para el testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train, corpus_test = train_test_split(corpus_filtrado, test_size=0.2)\n",
    "\n",
    "print (\"Cantidad de tweets en el conjunto de entrenamiento: \" + str(len(corpus_train)))\n",
    "print (\"Cantidad de tweets en el conjunto de testeo: \" + str(len(corpus_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Clasificador binario con tokenizador y POS tag de Freeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez separados el conjunto de train y test procedemos a tokenizar los tweets provenientes del conjunto train. Para esto usaremos la librería Freeling y NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(corpus_train)):\n",
    "    try:\n",
    "        print corpus_train.loc[i, :]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/opt/local/bin/python3.2\n",
    "# -*- encoding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import freeling\n",
    "import codecs\n",
    "import re\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "FREELINGDIR = \"/usr/local\"\n",
    "DATA = FREELINGDIR+\"/share/freeling/\"\n",
    "LANG=\"es\"\n",
    "print \"hola 1\"\n",
    "freeling.util_init_locale(\"default\") \n",
    "la = freeling.lang_ident(DATA + \"common/lang_ident/ident.dat\")\n",
    "\n",
    "print \"hola 2\"\n",
    "\n",
    "op = freeling.maco_options(\"es\")\n",
    "op.set_data_files(\"\",\n",
    "                  DATA+LANG+\"common/punct.dat\",\n",
    "                  DATA+LANG+\"/dicc.src\", \n",
    "                  DATA+LANG+\"/afixos.dat\",\n",
    "                  \"\",\n",
    "                  DATA+LANG+\"/locucions.dat\",\n",
    "                  DATA+LANG+\"/np.dat\",\n",
    "                  DATA+LANG+\"/quantities.dat\",\n",
    "                  DATA+LANG+\"/probabilitats.dat\")\n",
    "#op.set_retok_contractions(False)\n",
    "print \"hola 3\"\n",
    "#lg  = freeling.lang_ident(DATA+\"common/lang_ident/ident-few.dat\")\n",
    "mf  = freeling.maco(op)\n",
    "tk  = freeling.tokenizer(DATA+LANG+\"/tokenizer.dat\")\n",
    "sp  = freeling.splitter(DATA+LANG+\"/splitter.dat\")\n",
    "sid = sp.open_session()\n",
    "\n",
    "print \"hola 4\"\n",
    "#tg  = freeling.hmm_tagger(DATA+LANG+\"/tagger.dat\",1,2)\n",
    "#sen = freeling.senses(DATA+LANG+\"/senses.dat\");\n",
    "#ukb = freeling.ukb(DATA+LANG+\"/ukb.dat\")\n",
    "\n",
    "\n",
    "def tag (obj):\n",
    "    sent = obj[\"text\"]\n",
    "    out = obj\n",
    "    lang = lg.identify_language(sent)\n",
    "    l = tk.tokenize(sent)\n",
    "    ls = sp.split(l,1) # old value 0\n",
    "    ls = mf.analyze(ls)\n",
    "    ls = tg.analyze(ls)\n",
    "    wss = []\n",
    "    for s in ls:\n",
    "        ws = s.get_words()\n",
    "        for w in ws:\n",
    "            an = w.get_analysis()\n",
    "            a = an[0]\n",
    "            wse = dict(wordform =  w.get_form(),\n",
    "                       lemma = a.get_lemma(),\n",
    "                       tag = a.get_tag(),\n",
    "                       prob = a.get_prob(),\n",
    "                       analysis = len(an))\n",
    "            wss.append(wse)\n",
    "    out['words'] = wss\n",
    "    out['lang'] = lang\n",
    "    return out\n",
    "\n",
    "tweets = []\n",
    "print \"hola 5\"\n",
    "#for i in range(1, len(corpus_train)):\n",
    "#    try:\n",
    "#        tweets.append(tag(corpus_train.loc[i, :]))\n",
    "#    except:\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1- Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "os.environ[\"ANALYZER\"] = \"/home/santiago/Descargas/FreeLing-4.0/src/main\"\n",
    "os.environ[\"FREELINGSHARE\"] = \"/home/santiago/Descargas/FreeLing-4.0/data/es\"\n",
    "comando = os.environ[\"ANALYZER\"] + \"/analyzer -f \" + \"/home/santiago/Descargas/FreeLing-4.0/data\"\n",
    "\n",
    "def POS_tagging(datos, largo):\n",
    "    listaTuplas = []\n",
    "    \n",
    "    # Retorna una lista de tuplas. \n",
    "    # Cada tupla posee un diccionario (dict) palabra-frecuencia del comentario y la clasificación asociada\n",
    "    # En otras palabras [(dict1, clasificacion1),(dict2, clasificacion2), ... ]\n",
    "\n",
    "    # Se recorren los comentarios y para cada uno de ellos se tokeniza con nltk\n",
    "    for i in range(1, len(datos)):\n",
    "        try:\n",
    "            # Se crea el diccionario asociado al comentario\n",
    "            dic = {}\n",
    "            \n",
    "            # Por cada palabra retornada de la tokenizacion del comentario\n",
    "            p = Popen(comando, shell = True, stdout=PIPE, stdin=PIPE, stderr=STDOUT)\n",
    "            stdout = p.communicate(input=datos.loc[i, \"text\"].encode('ascii','ignore'))[0]\n",
    "            for linea in stdout.decode().split('\\r\\n'):\n",
    "                print linea\n",
    "                token = linea.split(' ')\n",
    "                tag = token[2]\n",
    "                palabra = token[0]\n",
    "                print i\n",
    "                print palabra\n",
    "                \n",
    "                if((tag[0:1] != 'F') and (tag[0:2] != 'RG') and (tag[0:2] != 'DP') and (tag[0:2] != 'DT') and (tag[0:2] != 'DE') \n",
    "                    and (tag[0:2] != 'DA') and (tag[0:1] != 'N') and (tag[0:2] != 'RG') and (tag[0:2] != 'PP') and (tag[0:2] != 'PD')\n",
    "                    and (tag[0:2] != 'PX') and (tag[0:2] != 'PT') and (tag[0:2] != 'PR') and (tag[0:2] != 'PE') and (tag[0:1] != 'I')\n",
    "                    and (tag[0:1] != 'S') and (tag[0:1] != 'Z') and (tag[0:1] != 'W')):\n",
    "\n",
    "                    # Si la palabra está en el diccionario del comentario, se aumenta la frecuencia\n",
    "                    # En caso contrario se la pone en el diccionario con valor 1\n",
    "                    if(palabra.lower() in dic): \n",
    "                        dic[palabra.lower()] = dic[palabra.lower()] + 1\n",
    "                    else:\n",
    "                        dic[palabra.lower()] = 1\n",
    "\n",
    "                    # Luego de tokenizado el comentario, se agrega una tupla a la lista que contendrá\n",
    "                    # el diccionario de frecuencias y la clasificaion asociada al comentario\n",
    "            listaTuplas.insert(i,(dic,datos.loc[i, \"humoristico\"]))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return listaTuplas\n",
    "\n",
    "datos_test_tokenizados_pos_tagging = POS_tagging(corpus_train, len(corpus_filtrado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for i in os.environ:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in datos_test_tokenizados_pos_tagging[0][0]:\n",
    "    print (str(x) + ':' + str(datos_test_tokenizados_pos_tagging[0][0][x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
